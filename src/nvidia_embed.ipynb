{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import DataParallel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import pymupdf\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = AutoModel.from_pretrained(\"nvidia/NV-Embed-v2\", trust_remote_code=True)\n",
    "# embedding_model.to(device)\n",
    "\n",
    "dir_path = \"/secure/shared_data/rag_embedding_model\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NV-Embed-v2\", trust_remote_code=True, cache_dir=dir_path)\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = AutoModel.from_pretrained(\"nvidia/NV-Embed-v2\", trust_remote_code=True, cache_dir=dir_path)\n",
    "\n",
    "# Move the model to the device (GPU or CPU)\n",
    "embedding_model.to(device)\n",
    "embedding_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Each query needs to be accompanied by an corresponding instruction describing the task.\n",
    "# task_name_to_instruct = {\"example\": \"Given a question, retrieve passages that answer the question\",}\n",
    "\n",
    "# query_prefix = \"Instruct: \"+task_name_to_instruct[\"example\"]+\"\\nQuery: \"\n",
    "# queries = [\n",
    "#     'are judo throws allowed in wrestling?', \n",
    "#     'how to become a radiology technician in michigan?'\n",
    "#     ]\n",
    "\n",
    "# # No instruction needed for retrieval passages\n",
    "# passage_prefix = \"\"\n",
    "# passages = [\n",
    "#     \"Since you're reading this, you are probably someone from a judo background or someone who is just wondering how judo techniques can be applied under wrestling rules. So without further ado, let's get to the question. Are Judo throws allowed in wrestling? Yes, judo throws are allowed in freestyle and folkstyle wrestling. You only need to be careful to follow the slam rules when executing judo throws. In wrestling, a slam is lifting and returning an opponent to the mat with unnecessary force.\",\n",
    "#     \"Below are the basic steps to becoming a radiologic technologist in Michigan:Earn a high school diploma. As with most careers in health care, a high school education is the first step to finding entry-level employment. Taking classes in math and science, such as anatomy, biology, chemistry, physiology, and physics, can help prepare students for their college studies and future careers.Earn an associate degree. Entry-level radiologic positions typically require at least an Associate of Applied Science. Before enrolling in one of these degree programs, students should make sure it has been properly accredited by the Joint Review Committee on Education in Radiologic Technology (JRCERT).Get licensed or certified in the state of Michigan.\"\n",
    "# ]\n",
    "\n",
    "# # get the embeddings\n",
    "# max_length = 32768\n",
    "# query_embeddings = embedding_model.encode(queries, instruction=query_prefix, max_length=max_length)\n",
    "# passage_embeddings = embedding_model.encode(passages, instruction=passage_prefix, max_length=max_length)\n",
    "\n",
    "# # normalize embeddings\n",
    "# query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "# passage_embeddings = F.normalize(passage_embeddings, p=2, dim=1)\n",
    "\n",
    "# scores = (query_embeddings @ passage_embeddings.T) * 100\n",
    "# print(scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(files=\"/home/yl3427/cylab/rag_tnm/selfCorrectionAgent/ajcc_7thed_cancer_staging_manual.pdf\"):\n",
    "    if not isinstance(files, list):\n",
    "        files = [files]  \n",
    "\n",
    "    documents = []\n",
    "    for file_path in files:\n",
    "        doc = pymupdf.open(file_path)\n",
    "        text = \"\"\n",
    "        \n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "\n",
    "        text = group_broken_paragraphs(text)\n",
    "        text = clean_extra_whitespace_within_paragraphs(text)\n",
    "\n",
    "        document = Document(\n",
    "            page_content=text,\n",
    "            metadata={\"source\": file_path}\n",
    "        )\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    return documents\n",
    "\n",
    "def clean_extra_whitespace_within_paragraphs(text):\n",
    "    return re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "def group_broken_paragraphs(text):\n",
    "    text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "    # text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_docs_tokens(docs_processed, tokenizer):\n",
    "    lengths = [len(tokenizer.encode(doc.page_content)) for doc in docs_processed]\n",
    "    print(f\"Maximum sequence length in chunks: {max(lengths)}\")\n",
    "    fig = pd.Series(lengths).hist()\n",
    "    plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "    plt.show()\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base,\n",
    "    tokenizer\n",
    "):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        separators = [\"\\n\\n\", \"\\n\", '(?<=[.?\"\\s])\\s+', \" \"],\n",
    "        tokenizer=tokenizer,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=0,\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        is_separator_regex=True\n",
    "    )\n",
    "\n",
    "    docs_processed = (text_splitter.split_documents([doc]) for doc in knowledge_base)\n",
    "\n",
    "    unique_texts = set()\n",
    "    docs_processed_unique = []\n",
    "    for doc_chunk in docs_processed:\n",
    "        for doc in doc_chunk:\n",
    "            if doc.page_content not in unique_texts:\n",
    "                unique_texts.add(doc.page_content)\n",
    "                docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed = split_documents(\n",
    "    chunk_size = 512, \n",
    "    knowledge_base = documents,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_docs_tokens(docs_processed, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of chunks: {len(docs_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "\n",
    "def embed_docs_in_chroma(docs, collection):\n",
    "    \n",
    "    pbar = tqdm(total=len(docs))\n",
    "\n",
    "    for doc in docs:\n",
    "        id = str(doc.metadata[\"start_index\"])\n",
    "        doc_text = doc.page_content\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = embedding_model.encode([doc_text], max_length=MAX_LENGTH)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "\n",
    "        collection.add(\n",
    "            embeddings=embeddings,\n",
    "            # metadatas=[{}],\n",
    "            documents=[doc_text],\n",
    "            ids=[id]\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "# client = chromadb.Client()\n",
    "client = chromadb.PersistentClient(path=\"/home/yl3427/cylab/chroma_db\",\n",
    "                                   settings=Settings(allow_reset=True))\n",
    "\n",
    "brca_collection = client.get_or_create_collection(name = \"brca\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "print(brca_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_docs_in_chroma(docs_processed, brca_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잘 계산되나 시험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpheus_collection = client.create_collection(\n",
    "     name=\"morpheus\", metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "        \"This is your last chance. After this, there is no turning back.\",\n",
    "        \"You take the blue pill, the story ends, you wake up in your bed and believe whatever you want to believe.\",\n",
    "        \"You take the red pill, you stay in Wonderland, and I show you how deep the rabbit hole goes.\",\n",
    "    ]\n",
    "morpheus_collection.add(\n",
    "       documents=docs,\n",
    "    embeddings= embedding_model.encode(docs, max_length=MAX_LENGTH).detach().cpu().numpy().tolist(),\n",
    "    ids=[\"quote1\", \"quote2\", \"quote3\"],\n",
    ")\n",
    "\n",
    "morpheus_collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying by a set of query_texts\n",
    "queries = [\"Take the blow pill\", \"chance\", \"yewon\"]\n",
    "results = morpheus_collection.query(query_embeddings=embedding_model.encode(queries, max_length=MAX_LENGTH).detach().cpu().numpy().tolist(),\n",
    "                                                    include=[\"metadatas\", \"documents\", \"distances\"],\n",
    "                                                    n_results=2,\n",
    "                                                    )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in range(len(results['documents'])):\n",
    "    print(f\"For {query}st query: \")\n",
    "    for top in range(len(results['documents'][query])):\n",
    "        print(f\"----top {top}st----\")\n",
    "        print(results['documents'][query][top])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"empty\",\n",
    "                base_url = \"http://localhost:8000/v1\")\n",
    "\n",
    "def agent(client, prompt, output_schema):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        messages = messages,\n",
    "        extra_body={\"guided_json\":output_schema},\n",
    "        temperature = 0.1)\n",
    "  \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Return your reasoning and the T stage in the following JSON format:\n",
    "# {\n",
    "#   \"reasoning\": \"Step-by-step explanation of how you interpreted the report to determine the T stage.\",\n",
    "#   \"T_stage\": \"T1, T2, T3, or T4\"\n",
    "# }\n",
    "\n",
    "# Return your reasoning and the N stage in the following JSON format:\n",
    "# {\n",
    "#   \"reasoning\": \"Step-by-step explanation of how you interpreted the report to determine the N stage.\",\n",
    "#   \"N_stage\": \"N0, N1, N2, or N3\"\n",
    "# }\n",
    "\n",
    "class Response(BaseModel):\n",
    "    reasoning: str = Field(description=\"Step-by-step explanation of how you interpreted the report to determine the cancer stage.\")\n",
    "    stage: str = Field(description=\"The cancer stage determined from the report.\")\n",
    " \n",
    "schema = Response.model_json_schema()\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_query = '''Please infer a list of general rules that help predict the T stage for breast cancer based on the AJCC's TNM Staging System. Ensure there is at least one rule for each T stage (T1, T2, T3, T4) in the list of rules.'''\n",
    "# main_query = '''Please infer a list of general rules that help predict the N stage for breast cancer based on the AJCC's TNM Staging System. Ensure there is at least one rule for each N stage (N0, N1, N2, N3) in the list of rules.'''\n",
    "\n",
    "query_decomposer_prompt = \"\"\"\n",
    "You are a helpful assistant that decomposes an input query into multiple sub-queries.\n",
    "Your goal is to break down the input into a set of specific sub-questions that can be answered individually to cover the full scope of the original question.\n",
    "\n",
    "Generate at least 5 sub-queries related to the following input query: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "rule_generator_prompt = \"\"\"\n",
    "You are a helpful assistant. Based on the provided context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = agent(client, main_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = agent(client, query_decomposer_prompt.format(question=main_query))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subqueries = answer.split(\"\\n\")\n",
    "subqueries = [subquery.strip() for subquery in subqueries]\n",
    "subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = [main_query]\n",
    "queries = subqueries\n",
    "results = brca_collection.query(query_embeddings=embedding_model.encode(queries, max_length=MAX_LENGTH).detach().cpu().numpy().tolist(),\n",
    "                                                    include=[\"metadatas\", \"documents\", \"distances\"],\n",
    "                                                    n_results=1,\n",
    "                                                    )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_context = \"\"\n",
    "id_set = set()\n",
    "for query in range(len(results['documents'])):\n",
    "    # print(f\"For {query}st query: \")\n",
    "    for top in range(len(results['documents'][query])):\n",
    "        if results['ids'][query][top] in id_set:\n",
    "            print(f\"Skip at {query}, {top}\")\n",
    "            continue\n",
    "        else:\n",
    "            id_set.add(results['ids'][query][top])\n",
    "            retrieved_context += results['documents'][query][top]+\"\\n\"\n",
    "            print(f\"Add at {query}, {top}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = agent(client, rule_generator_prompt.format(context = retrieved_context, question = main_query))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

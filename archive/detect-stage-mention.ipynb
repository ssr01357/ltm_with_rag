{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.prompts import system_instruction\n",
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.metrics import t14_performance_report, n03_performance_report, m01_performance_report\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# initialized client\n",
    "client = InferenceClient(model=\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MED42_PROMPT_TEMPLATE = \"\"\"<|system|>:{system_instruction}\n",
    "<|prompter|>:{prompt}\n",
    "<|assistant|>:\"\"\"\n",
    "\n",
    "# T_stage_mention_detecting_prompt = \"\"\"You are provided with a pathology report for a cancer patient. \n",
    "# Please review this report. \n",
    "\n",
    "# Here is the report:\n",
    "# ```\n",
    "# {report}\n",
    "# ```\n",
    "\n",
    "# Is the pathologic T (pT) stage of the patient's cancer mentioned in the report? Please answer only Yes or No.\n",
    "# \"\"\"\n",
    "\n",
    "T_stage_mention_detecting_prompt = \"\"\"You are provided with a pathology report for a cancer patient. \n",
    "\n",
    "Here is the report:\n",
    "```\n",
    "{report}\n",
    "```\n",
    "\n",
    "Is the T stage explicitly mentioned in the provided pathology report? Please answer only Yes or No.\n",
    "\"\"\"\n",
    "\n",
    "N_stage_mention_detecting_prompt = \"\"\"You are provided with a pathology report for a cancer patient. \n",
    "\n",
    "Here is the report:\n",
    "```\n",
    "{report}\n",
    "```\n",
    "\n",
    "Is the N stage explicitly mentioned in the provided pathology report? Please answer only Yes or No.\n",
    "\"\"\"\n",
    "\n",
    "M_stage_mention_detecting_prompt = \"\"\"You are provided with a pathology report for a cancer patient. \n",
    "\n",
    "Here is the report:\n",
    "```\n",
    "{report}\n",
    "```\n",
    "\n",
    "Is the M stage explicitly mentioned in the provided pathology report? Please answer only Yes or No.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRCA T Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/146 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [03:05<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "t14_brca_testing = pd.read_csv(\"/secure/shared_data/tcga_path_reports/t14_data/BRCA_T14_testing.csv\")\n",
    "\n",
    "pbar = tqdm(total=t14_brca_testing.shape[0])\n",
    "ans_list = []\n",
    "for _, report in t14_brca_testing.iterrows():\n",
    "\n",
    "    filled_text_prompt = T_stage_mention_detecting_prompt.format(report=report[\"text\"])\n",
    "    lm_formated_prompt = MED42_PROMPT_TEMPLATE.format(system_instruction=system_instruction, prompt=filled_text_prompt)\n",
    "    decoded_ans = client.text_generation(prompt=lm_formated_prompt, do_sample=False, \n",
    "                                             max_new_tokens=12)\n",
    "    ans_list.append(decoded_ans)\n",
    "\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t14_brca_testing[\"stage_mention\"] = ans_list\n",
    "sum(t14_brca_testing[\"stage_mention\"].str.contains('No', case=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRCA: (146, 8)\n",
      "(109, 8) (37, 8)\n"
     ]
    }
   ],
   "source": [
    "t14_ZSCOT_path = \"/secure/shared_data/tnm/t14_res/med42-t0.7-tp0.95-nrs1.csv\"\n",
    "# load the ZS-COT results\n",
    "t14_ZSCOT_df = pd.read_csv(t14_ZSCOT_path)\n",
    "t14_ZSCOT_brca = t14_ZSCOT_df.merge(t14_brca_testing[[\"patient_filename\", \"stage_mention\"]], on=\"patient_filename\")\n",
    "print(\"BRCA:\", t14_ZSCOT_brca.shape)\n",
    "\n",
    "mention_flag = t14_ZSCOT_brca[\"stage_mention\"].str.contains('No', case=False)\n",
    "t14_ZSCOT_brca_hasMention = t14_ZSCOT_brca[~mention_flag]\n",
    "t14_ZSCOT_brca_noMention = t14_ZSCOT_brca[mention_flag]\n",
    "print(t14_ZSCOT_brca_hasMention.shape, t14_ZSCOT_brca_noMention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          T1       0.87      0.83      0.85        24\n",
      "          T2       0.91      0.92      0.91        63\n",
      "          T3       0.69      0.75      0.72        12\n",
      "          T4       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.87       101\n",
      "   macro avg       0.87      0.75      0.79       101\n",
      "weighted avg       0.87      0.87      0.87       101\n",
      "\n",
      "Precision: 0.8064409961214533 (CI: 0.5695539941829004 0.9305555555555556 )\n",
      "Recall: 0.765293477705456 (CI: 0.58003484249916 0.9275659602283293 )\n",
      "F1: 0.773263615784203 (CI: 0.5712708914415336 0.9155401640890521 )\n"
     ]
    }
   ],
   "source": [
    "_ = t14_performance_report(t14_ZSCOT_brca_hasMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          T1       0.67      0.57      0.62         7\n",
      "          T2       0.83      0.83      0.83        24\n",
      "          T3       0.00      0.00      0.00         1\n",
      "          T4       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.74        34\n",
      "   macro avg       0.50      0.48      0.49        34\n",
      "weighted avg       0.75      0.74      0.74        34\n",
      "\n",
      "Precision: 0.48305864157892064 (CI: 0.3079967948717949 0.6955946969696968 )\n",
      "Recall: 0.46481297095845037 (CI: 0.2636838161838162 0.7084953703703704 )\n",
      "F1: 0.45943471844685174 (CI: 0.281437106918239 0.6542196998480242 )\n"
     ]
    }
   ],
   "source": [
    "_ = t14_performance_report(t14_ZSCOT_brca_noMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_ZSCOT_brca_noMention.to_csv(\"/secure/shared_data/rag_tnm_results/t14_results/brca_t14_noMention.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRCA N Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [02:31<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "n03_brca_testing = pd.read_csv(\"/secure/shared_data/tcga_path_reports/n03_data/BRCA_N03_testing.csv\")\n",
    "\n",
    "pbar = tqdm(total=n03_brca_testing.shape[0])\n",
    "ans_list = []\n",
    "for _, report in n03_brca_testing.iterrows():\n",
    "\n",
    "    filled_text_prompt = N_stage_mention_detecting_prompt.format(report=report[\"text\"])\n",
    "    lm_formated_prompt = MED42_PROMPT_TEMPLATE.format(system_instruction=system_instruction, prompt=filled_text_prompt)\n",
    "    decoded_ans = client.text_generation(prompt=lm_formated_prompt, do_sample=False, \n",
    "                                             max_new_tokens=12)\n",
    "    ans_list.append(decoded_ans)\n",
    "\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n03_brca_testing[\"stage_mention\"] = ans_list\n",
    "sum(n03_brca_testing[\"stage_mention\"].str.contains('No', case=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRCA: (131, 8)\n",
      "(82, 8) (49, 8)\n"
     ]
    }
   ],
   "source": [
    "n03_ZSCOT_path = \"/secure/shared_data/tnm/n03_res/med42-t0.7-tp0.95-nrs1.csv\"\n",
    "# load the ZS-COT results\n",
    "n03_ZSCOT_df = pd.read_csv(n03_ZSCOT_path)\n",
    "n03_ZSCOT_brca = n03_ZSCOT_df.merge(n03_brca_testing[[\"patient_filename\", \"stage_mention\"]], on=\"patient_filename\")\n",
    "print(\"BRCA:\", n03_ZSCOT_brca.shape)\n",
    "\n",
    "mention_flag = n03_ZSCOT_brca[\"stage_mention\"].str.contains('No', case=False)\n",
    "n03_ZSCOT_brca_hasMention = n03_ZSCOT_brca[~mention_flag]\n",
    "n03_ZSCOT_brca_noMention = n03_ZSCOT_brca[mention_flag]\n",
    "print(n03_ZSCOT_brca_hasMention.shape, n03_ZSCOT_brca_noMention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       1.00      0.94      0.97        16\n",
      "          N1       0.92      0.90      0.91        39\n",
      "          N2       0.62      0.80      0.70        10\n",
      "          N3       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.87        75\n",
      "   macro avg       0.83      0.83      0.83        75\n",
      "weighted avg       0.88      0.87      0.87        75\n",
      "\n",
      "Precision: 0.8264161501966703 (CI: 0.730248006566604 0.9132142857142856 )\n",
      "Recall: 0.8310111636678434 (CI: 0.7247888661635478 0.9202989483634645 )\n",
      "F1: 0.8189161115757877 (CI: 0.7201918956439536 0.9057276600819024 )\n"
     ]
    }
   ],
   "source": [
    "_ = n03_performance_report(n03_ZSCOT_brca_hasMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       0.91      1.00      0.95        39\n",
      "          N1       1.00      0.25      0.40         4\n",
      "          N3       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.91        44\n",
      "   macro avg       0.64      0.42      0.45        44\n",
      "weighted avg       0.89      0.91      0.88        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def modifiled_n03_performance_report(df, ans_col=\"ans_str\"):\n",
    "    # check if the ans_col contain any valid prediction (e.g., T1, T2, T3, T4)\n",
    "    df['Has_Valid_Prediction'] = df[ans_col].str.contains('N0|N1|N2|N3', case=False)\n",
    "    # transform the prediction string to code\n",
    "    coded_pred_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        row[ans_col] = str(row[ans_col])\n",
    "        if \"N0\" in row[ans_col]:\n",
    "            coded_pred_list.append(0)\n",
    "        elif \"N1\" in row[ans_col]:\n",
    "            coded_pred_list.append(1)\n",
    "        elif \"N2\" in row[ans_col]:\n",
    "            coded_pred_list.append(2)\n",
    "        elif \"N3\" in row[ans_col]:\n",
    "            coded_pred_list.append(3)\n",
    "        else:\n",
    "            # unvalid answers \n",
    "            # Has_Valid_Prediction == False\n",
    "            coded_pred_list.append(-1)\n",
    "    df['coded_pred'] = coded_pred_list\n",
    "\n",
    "    effective_index = df[\"Has_Valid_Prediction\"] == True\n",
    "    coded_pred = df[effective_index]['coded_pred'].to_list()\n",
    "    n_labels = df[effective_index][\"n\"].to_list()\n",
    "\n",
    "    target_names = ['N0', 'N1', 'N3']\n",
    "    print(classification_report(n_labels, coded_pred, target_names=target_names))\n",
    "\n",
    "_ = modifiled_n03_performance_report(n03_ZSCOT_brca_noMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n03_ZSCOT_brca_noMention.to_csv(\"/secure/shared_data/rag_tnm_results/n03_results/brca_n03_noMention.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRCA M Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [02:40<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "m01_brca_testing = pd.read_csv(\"/secure/shared_data/tcga_path_reports/m01_data/BRCA_M01_testing.csv\")\n",
    "\n",
    "pbar = tqdm(total=m01_brca_testing.shape[0])\n",
    "ans_list = []\n",
    "for _, report in m01_brca_testing.iterrows():\n",
    "\n",
    "    filled_text_prompt = M_stage_mention_detecting_prompt.format(report=report[\"text\"])\n",
    "    lm_formated_prompt = MED42_PROMPT_TEMPLATE.format(system_instruction=system_instruction, prompt=filled_text_prompt)\n",
    "    decoded_ans = client.text_generation(prompt=lm_formated_prompt, do_sample=False, \n",
    "                                             max_new_tokens=12)\n",
    "    ans_list.append(decoded_ans)\n",
    "\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m01_brca_testing[\"stage_mention\"] = ans_list\n",
    "sum(m01_brca_testing[\"stage_mention\"].str.contains('No', case=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRCA: (138, 18)\n",
      "(33, 18) (105, 18)\n"
     ]
    }
   ],
   "source": [
    "tmp1 = pd.read_csv(\"/secure/shared_data/tnm/m01_res/med42-t0.7-tp0.95-nrs5_batch1.csv\")\n",
    "tmp2 = pd.read_csv(\"/secure/shared_data/tnm/m01_res/med42-t0.7-tp0.95-nrs5_batch2.csv\")\n",
    "m01_ZSCOT_df = pd.concat([tmp1, tmp2])\n",
    "# load the ZS-COT results\n",
    "m01_ZSCOT_brca = m01_ZSCOT_df.merge(m01_brca_testing[[\"patient_filename\", \"stage_mention\"]], on=\"patient_filename\")\n",
    "print(\"BRCA:\", m01_ZSCOT_brca.shape)\n",
    "\n",
    "mention_flag = m01_ZSCOT_brca[\"stage_mention\"].str.contains('No', case=False)\n",
    "m01_ZSCOT_brca_hasMention = m01_ZSCOT_brca[~mention_flag]\n",
    "m01_ZSCOT_brca_noMention = m01_ZSCOT_brca[mention_flag]\n",
    "print(m01_ZSCOT_brca_hasMention.shape, m01_ZSCOT_brca_noMention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn=14, fp=16, fn=1, tp=1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          M0       0.93      0.47      0.62        30\n",
      "          M1       0.06      0.50      0.11         2\n",
      "\n",
      "    accuracy                           0.47        32\n",
      "   macro avg       0.50      0.48      0.36        32\n",
      "weighted avg       0.88      0.47      0.59        32\n",
      "\n",
      "Precision: 0.49835754449251957 (CI: 0.4075994318181818 0.585906862745098 )\n",
      "Recall: 0.4524569624204844 (CI: 0.171875 0.8064516129032258 )\n",
      "F1: 0.3628455002637719 (CI: 0.23809523809523808 0.5174603174603173 )\n"
     ]
    }
   ],
   "source": [
    "_ = m01_performance_report(m01_ZSCOT_brca_hasMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn=68, fp=32, fn=1, tp=2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          M0       0.99      0.68      0.80       100\n",
      "          M1       0.06      0.67      0.11         3\n",
      "\n",
      "    accuracy                           0.68       103\n",
      "   macro avg       0.52      0.67      0.46       103\n",
      "weighted avg       0.96      0.68      0.78       103\n",
      "\n",
      "Precision: 0.5225604985685791 (CI: 0.4855072463768116 0.5694444444444444 )\n",
      "Recall: 0.6656147667850195 (CI: 0.321050418148611 0.8706757425742574 )\n",
      "F1: 0.45516168658343753 (CI: 0.38690476190476186 0.5433136912205315 )\n"
     ]
    }
   ],
   "source": [
    "_ = m01_performance_report(m01_ZSCOT_brca_noMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUAD T Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [02:01<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "t14_luad_testing = pd.read_csv(\"/secure/shared_data/tcga_path_reports/t14_data/LUAD_T14_testing.csv\")\n",
    "\n",
    "pbar = tqdm(total=t14_luad_testing.shape[0])\n",
    "ans_list = []\n",
    "for _, report in t14_luad_testing.iterrows():\n",
    "\n",
    "    filled_text_prompt = T_stage_mention_detecting_prompt.format(report=report[\"text\"])\n",
    "    lm_formated_prompt = MED42_PROMPT_TEMPLATE.format(system_instruction=system_instruction, prompt=filled_text_prompt)\n",
    "    decoded_ans = client.text_generation(prompt=lm_formated_prompt, do_sample=False, \n",
    "                                             max_new_tokens=12)\n",
    "    ans_list.append(decoded_ans)\n",
    "\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t14_luad_testing[\"stage_mention\"] = ans_list\n",
    "sum(t14_luad_testing[\"stage_mention\"].str.contains('No', case=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUAD: (77, 8)\n",
      "(60, 8) (17, 8)\n"
     ]
    }
   ],
   "source": [
    "t14_ZSCOT_path = \"/secure/shared_data/tnm/t14_res/med42-t0.7-tp0.95-nrs1.csv\"\n",
    "# load the ZS-COT results\n",
    "t14_ZSCOT_df = pd.read_csv(t14_ZSCOT_path)\n",
    "t14_ZSCOT_luad = t14_ZSCOT_df.merge(t14_luad_testing[[\"patient_filename\", \"stage_mention\"]], on=\"patient_filename\")\n",
    "print(\"LUAD:\", t14_ZSCOT_luad.shape)\n",
    "\n",
    "mention_flag = t14_ZSCOT_luad[\"stage_mention\"].str.contains('No', case=False)\n",
    "t14_ZSCOT_luad_hasMention = t14_ZSCOT_luad[~mention_flag]\n",
    "t14_ZSCOT_luad_noMention = t14_ZSCOT_luad[mention_flag]\n",
    "print(t14_ZSCOT_luad_hasMention.shape, t14_ZSCOT_luad_noMention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          T1       1.00      0.95      0.97        19\n",
      "          T2       1.00      1.00      1.00        28\n",
      "          T3       0.90      1.00      0.95         9\n",
      "          T4       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.97      0.99      0.98        60\n",
      "weighted avg       0.98      0.98      0.98        60\n",
      "\n",
      "Precision: 0.9739981782242813 (CI: 0.9166666666666666 1.0 )\n",
      "Recall: 0.9861578083897027 (CI: 0.952734375 1.0 )\n",
      "F1: 0.9781584380773338 (CI: 0.9283263305322129 1.0 )\n"
     ]
    }
   ],
   "source": [
    "_ = t14_performance_report(t14_ZSCOT_luad_hasMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          T1       0.50      0.25      0.33         4\n",
      "          T2       0.56      0.71      0.63         7\n",
      "          T3       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.35      0.32      0.32        12\n",
      "weighted avg       0.49      0.50      0.48        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def modified_t14_performance_report(df, ans_col=\"ans_str\"):\n",
    "    # check if the ans_col contain any valid prediction (e.g., T1, T2, T3, T4)\n",
    "    df['Has_Valid_Prediction'] = df[ans_col].str.contains('T1|T2|T3|T4', case=False)\n",
    "    # transform the prediction string to code\n",
    "    # note that following the t column we set T1 = 0, ... T4 = 3 \n",
    "    coded_pred_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        if \"T1\" in row[ans_col]:\n",
    "            coded_pred_list.append(0)\n",
    "        elif \"T2\" in row[ans_col]:\n",
    "            coded_pred_list.append(1)\n",
    "        elif \"T3\" in row[ans_col]:\n",
    "            coded_pred_list.append(2)\n",
    "        elif \"T4\" in row[ans_col]:\n",
    "            coded_pred_list.append(3)\n",
    "        else:\n",
    "            # unvalid answers \n",
    "            # Has_Valid_Prediction == False\n",
    "            coded_pred_list.append(-1)\n",
    "    df['coded_pred'] = coded_pred_list\n",
    "\n",
    "    effective_index = df[\"Has_Valid_Prediction\"] == True\n",
    "    coded_pred = df[effective_index]['coded_pred'].to_list()\n",
    "    t_labels = df[effective_index][\"t\"].to_list()\n",
    "\n",
    "    target_names = ['T1', 'T2', 'T3']\n",
    "    print(classification_report(t_labels, coded_pred, target_names=target_names))\n",
    "\n",
    "modified_t14_performance_report(t14_ZSCOT_luad_noMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t14_ZSCOT_luad_noMention.to_csv(\"/secure/shared_data/rag_tnm_results/t14_results/luad_t14_noMention.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUAD N Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [02:36<00:00,  1.90s/it]\n"
     ]
    }
   ],
   "source": [
    "n03_luad_testing = pd.read_csv(\"/secure/shared_data/tcga_path_reports/n03_data/LUAD_N03_testing.csv\")\n",
    "\n",
    "pbar = tqdm(total=n03_luad_testing.shape[0])\n",
    "ans_list = []\n",
    "for _, report in n03_luad_testing.iterrows():\n",
    "\n",
    "    filled_text_prompt = N_stage_mention_detecting_prompt.format(report=report[\"text\"])\n",
    "    lm_formated_prompt = MED42_PROMPT_TEMPLATE.format(system_instruction=system_instruction, prompt=filled_text_prompt)\n",
    "    decoded_ans = client.text_generation(prompt=lm_formated_prompt, do_sample=False, \n",
    "                                             max_new_tokens=12)\n",
    "    ans_list.append(decoded_ans)\n",
    "\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n03_luad_testing[\"stage_mention\"] = ans_list\n",
    "sum(n03_luad_testing[\"stage_mention\"].str.contains('No', case=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUAD: (82, 8)\n",
      "(52, 8) (30, 8)\n"
     ]
    }
   ],
   "source": [
    "n03_ZSCOT_path = \"/secure/shared_data/tnm/n03_res/med42-t0.7-tp0.95-nrs1.csv\"\n",
    "# load the ZS-COT results\n",
    "n03_ZSCOT_df = pd.read_csv(n03_ZSCOT_path)\n",
    "n03_ZSCOT_luad = n03_ZSCOT_df.merge(n03_luad_testing[[\"patient_filename\", \"stage_mention\"]], on=\"patient_filename\")\n",
    "print(\"LUAD:\", n03_ZSCOT_luad.shape)\n",
    "\n",
    "mention_flag = n03_ZSCOT_luad[\"stage_mention\"].str.contains('No', case=False)\n",
    "n03_ZSCOT_luad_hasMention = n03_ZSCOT_luad[~mention_flag]\n",
    "n03_ZSCOT_luad_noMention = n03_ZSCOT_luad[mention_flag]\n",
    "print(n03_ZSCOT_luad_hasMention.shape, n03_ZSCOT_luad_noMention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       1.00      0.97      0.98        29\n",
      "          N1       0.86      1.00      0.92        12\n",
      "          N2       1.00      0.89      0.94         9\n",
      "          N3       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.96        51\n",
      "   macro avg       0.96      0.96      0.96        51\n",
      "weighted avg       0.97      0.96      0.96        51\n",
      "\n",
      "Precision: 0.9595605515893324 (CI: 0.9 1.0 )\n",
      "Recall: 0.9585178763715139 (CI: 0.8752638888888888 1.0 )\n",
      "F1: 0.954488209501788 (CI: 0.8842447017920595 1.0 )\n"
     ]
    }
   ],
   "source": [
    "_ = n03_performance_report(n03_ZSCOT_luad_hasMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          N0       1.00      0.95      0.98        22\n",
      "          N1       1.00      0.33      0.50         3\n",
      "          N2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88        25\n",
      "   macro avg       0.67      0.43      0.49        25\n",
      "weighted avg       1.00      0.88      0.92        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def modifiled_n03_performance_report(df, ans_col=\"ans_str\"):\n",
    "    # check if the ans_col contain any valid prediction (e.g., T1, T2, T3, T4)\n",
    "    df['Has_Valid_Prediction'] = df[ans_col].str.contains('N0|N1|N2|N3', case=False)\n",
    "    # transform the prediction string to code\n",
    "    coded_pred_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        row[ans_col] = str(row[ans_col])\n",
    "        if \"N0\" in row[ans_col]:\n",
    "            coded_pred_list.append(0)\n",
    "        elif \"N1\" in row[ans_col]:\n",
    "            coded_pred_list.append(1)\n",
    "        elif \"N2\" in row[ans_col]:\n",
    "            coded_pred_list.append(2)\n",
    "        elif \"N3\" in row[ans_col]:\n",
    "            coded_pred_list.append(3)\n",
    "        else:\n",
    "            # unvalid answers \n",
    "            # Has_Valid_Prediction == False\n",
    "            coded_pred_list.append(-1)\n",
    "    df['coded_pred'] = coded_pred_list\n",
    "\n",
    "    effective_index = df[\"Has_Valid_Prediction\"] == True\n",
    "    coded_pred = df[effective_index]['coded_pred'].to_list()\n",
    "    n_labels = df[effective_index][\"n\"].to_list()\n",
    "\n",
    "    target_names = ['N0', 'N1', 'N2']\n",
    "    print(classification_report(n_labels, coded_pred, target_names=target_names))\n",
    "\n",
    "_ = modifiled_n03_performance_report(n03_ZSCOT_luad_noMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n03_ZSCOT_luad_noMention.to_csv(\"/secure/shared_data/rag_tnm_results/n03_results/luad_n03_noMention.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUAD M Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:01<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "m01_luad_testing = pd.read_csv(\"/secure/shared_data/tcga_path_reports/m01_data/LUAD_M01_testing.csv\")\n",
    "\n",
    "pbar = tqdm(total=m01_luad_testing.shape[0])\n",
    "ans_list = []\n",
    "for _, report in m01_luad_testing.iterrows():\n",
    "\n",
    "    filled_text_prompt = M_stage_mention_detecting_prompt.format(report=report[\"text\"])\n",
    "    lm_formated_prompt = MED42_PROMPT_TEMPLATE.format(system_instruction=system_instruction, prompt=filled_text_prompt)\n",
    "    decoded_ans = client.text_generation(prompt=lm_formated_prompt, do_sample=False, \n",
    "                                             max_new_tokens=12)\n",
    "    ans_list.append(decoded_ans)\n",
    "\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m01_luad_testing[\"stage_mention\"] = ans_list\n",
    "sum(m01_luad_testing[\"stage_mention\"].str.contains('No', case=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRCA: (63, 18)\n",
      "(12, 18) (51, 18)\n"
     ]
    }
   ],
   "source": [
    "tmp1 = pd.read_csv(\"/secure/shared_data/tnm/m01_res/med42-t0.7-tp0.95-nrs5_batch1.csv\")\n",
    "tmp2 = pd.read_csv(\"/secure/shared_data/tnm/m01_res/med42-t0.7-tp0.95-nrs5_batch2.csv\")\n",
    "m01_ZSCOT_df = pd.concat([tmp1, tmp2])\n",
    "# load the ZS-COT results\n",
    "m01_ZSCOT_luad = m01_ZSCOT_df.merge(m01_luad_testing[[\"patient_filename\", \"stage_mention\"]], on=\"patient_filename\")\n",
    "print(\"BRCA:\", m01_ZSCOT_luad.shape)\n",
    "\n",
    "mention_flag = m01_ZSCOT_luad[\"stage_mention\"].str.contains('No', case=False)\n",
    "m01_ZSCOT_luad_hasMention = m01_ZSCOT_luad[~mention_flag]\n",
    "m01_ZSCOT_luad_noMention = m01_ZSCOT_luad[mention_flag]\n",
    "print(m01_ZSCOT_luad_hasMention.shape, m01_ZSCOT_luad_noMention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn=8, fp=2, fn=0, tp=2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          M0       1.00      0.80      0.89        10\n",
      "          M1       0.50      1.00      0.67         2\n",
      "\n",
      "    accuracy                           0.83        12\n",
      "   macro avg       0.75      0.90      0.78        12\n",
      "weighted avg       0.92      0.83      0.85        12\n",
      "\n",
      "Precision: 0.7511115079365079 (CI: 0.5 1.0 )\n",
      "Recall: 0.8422174963924965 (CI: 0.353125 1.0 )\n",
      "F1: 0.7494292861387726 (CI: 0.4135714285714286 1.0 )\n"
     ]
    }
   ],
   "source": [
    "_ = m01_performance_report(m01_ZSCOT_luad_hasMention, ans_col=\"ans_str_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn=35, fp=13, fn=2, tp=0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          M0       0.95      0.73      0.82        48\n",
      "          M1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.70        50\n",
      "   macro avg       0.47      0.36      0.41        50\n",
      "weighted avg       0.91      0.70      0.79        50\n",
      "\n",
      "Precision: 0.47412418082206487 (CI: 0.4342105263157895 0.5 )\n",
      "Recall: 0.36645461187328565 (CI: 0.3040019132653061 0.42857142857142855 )\n",
      "F1: 0.41257697382484215 (CI: 0.3708465189873418 0.45368490205446715 )\n"
     ]
    }
   ],
   "source": [
    "_ = m01_performance_report(m01_ZSCOT_luad_noMention, ans_col=\"ans_str_0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer-staging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
